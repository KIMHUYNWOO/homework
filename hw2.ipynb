{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# titanic_datatset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cv-05/anaconda3/envs/hy/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import wandb\n",
    "import argparse\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset): #Train 데이터\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X) #float 타입의 텐서 생성\n",
    "        self.y = torch.FloatTensor(y) #Long 타입의 텐서 생성\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.X) #텐서의 길이 반환\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx] #idx를 이용하여 input값인 feature를 x에 레이블 값인 target을 y에 선언\n",
    "        target = self.y[idx]\n",
    "        return {'input': feature, 'target': target}\n",
    "    def __str__(self):\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(len(self.X), self.X.shape, self.y.shape) #Data의 크기, feature의 shape, target의 shape를 str에 초기화\n",
    "        return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicTestDataset(Dataset): #Test 데이터\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.FloatTensor(X) \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        return {'input': feature}\n",
    "    def __str__(self):\n",
    "        str = \"Data Size: {0}, Input Shape: {1}\".format(len(self.X), self.X.shape)\n",
    "        return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(all_df): \n",
    "    # Pclass별 Fare 평균값을 사용하여 Fare 결측치 메우기\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index() #Pclass열을 기준으로 Fare열의 평균값을 계산하여 인덱스를 재설정\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"] #Fare_mean의 열 이름을 Pclass와 Fare_mean으로 변경\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")  #all_df데이터 프레임과 Fare_mean 열을 Pclass 열을 기준으로 병합\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]  #Fare열의 결측치가 있는 행을 선택하고 그 행의 Fare값을 Fare_mean의 값을 대입\n",
    "    \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # name을 세 개의 컬럼으로 분리하여 다시 all_df에 합침\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True) #Name열의 데이터를 ,.를 기준으로 최대 2번 분할하고 새로운 데이터로 저장\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"] #분할한 데이터의 열 이름 지정\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip() #문자열의 앞뒤 공백 제거\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1) #all_df와 name_df를 수평으로 concat\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # honorific별 Age 평균값을 사용하여 Age 결측치 메우기\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index() #name의 존칭인 honorific과 Age를 그룹지어 중앙값 계산후 반올림하여 honorific_age_mean에 젖아\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"] #Age의 결측치를 honorific_age_mean으로 채움 \n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1) #all_df에 병합된 honorific_age_mean열을 제거\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족수(family_num) 컬럼 새롭게 추가\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"] #부모/자녀수와 형제자매/배우자 수를 더한 새로운 열 family_num 추가\n",
    "\n",
    "    # 혼자탑승(alone) 컬럼 새롭게 추가\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1 #alone이라는 열을 생성하고 family_name이 0인 경우 1값을 저장\n",
    "    all_df[\"alone\"].fillna(0, inplace=True) # 결츨칙를 0으로 처리\n",
    "\n",
    "    # 학습에 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1) \n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # honorific 값 개수 줄이기\n",
    "    all_df.loc[\n",
    "    ~(\n",
    "            (all_df[\"honorific\"] == \"Mr\") |\n",
    "            (all_df[\"honorific\"] == \"Miss\") |\n",
    "            (all_df[\"honorific\"] == \"Mrs\") |\n",
    "            (all_df[\"honorific\"] == \"Master\")\n",
    "    ),\n",
    "    \"honorific\"\n",
    "    ] = \"other\" #주어진 존칭이 아닌 경우 other honorific 값 저장\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True) #탑승지의 결측치들은 missing으로 처리\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_6(all_df):\n",
    "  # 카테고리 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
    "  category_features = all_df.columns[all_df.dtypes == \"object\"] #all_df의 열의 데이터 타입이 object인 열들을 저장\n",
    "  from sklearn.preprocessing import LabelEncoder #문자열 데이터를 숫자로 인코딩하는 싸이킷 라이브러리\n",
    "  for category_feature in category_features: \n",
    "    le = LabelEncoder()\n",
    "    if all_df[category_feature].dtypes == \"object\":\n",
    "      le = le.fit(all_df[category_feature]) \n",
    "      all_df[category_feature] = le.transform(all_df[category_feature]) #문자열 값을 정수 레이블로 변환\n",
    "\n",
    "  return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset():\n",
    "    CURRENT_FILE_PATH = os.path.dirname(os.path.abspath('./hw2.ipynb')) #hw2_titani.ipynb가 있는 파일의 경로를 저장\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\") #train.csv가 있는 경로 저장\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\") #test.csv가 있는 경로 저장\n",
    "    \n",
    "    train_df = pd.read_csv(train_data_path) #불러오기\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False) #train_df와 test_df concat 열의 순서는 유지\n",
    "    all_df = get_preprocessed_dataset_1(all_df) #데이터 전처리 함수1~6\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "    \n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True) #train_data의 레이블 값인 Survived가 결측치가 아닌 행을 선택해 input값인 feature를 x에 저장\n",
    "    train_y = train_df[\"Survived\"] #y는 레이블 값을 저장\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True) #test_data에서 레이블 값이 결측치인 행을 선택해 test데이터의 input값을 저장\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    #print(dataset)\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2]) #train_data를 8:2비율로 학습 데이터와 검증 데이터로 분리\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "    #print(test_dataset)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class MyModel(nn.Module): #학습 모델 정의\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__() \n",
    "        self.model = nn.Sequential( #레이어를 연속적으로 쌓음\n",
    "            nn.Linear(n_input, 30),  #선형 레이어를 정의 뉴런의 개수는 30\n",
    "            nn.LeakyReLU(0.1), #LeakyReLU 활성화 함수\n",
    "            nn.Linear(30, 30), #총 2개의 뉴런 레이어층을 가짐\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(30, n_output),\n",
    "        )\n",
    "    def forward(self, x): #순전파 과정 순서대로 레이어릁 통과\n",
    "        x = self.model(x) #모델을 통과한 값 x를 저장\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_optimizer():\n",
    "  my_model = MyModel(n_input=11, n_output=2)\n",
    "  optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate) #SGD optim을 이용하여 weight를 업데이트함\n",
    "\n",
    "  return my_model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset() #학습,검증,테스트 데이터를 전처리후 불러오는 함수\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True) #batch_size가 16으로 한벅 반복 할 경우 16개씩 묶어서 학습을 하게됨\n",
    "validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader, test_data_loader):\n",
    "  n_epochs = wandb.config.epochs\n",
    "  loss_fn = nn.MSELoss()  # Use a built-in loss function\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss_train = 0.0\n",
    "    num_trains = 0\n",
    "    for train_batch in train_data_loader:\n",
    "      output_train = model(train_batch['input'])\n",
    "      output_train = torch.argmax(output_train, dim=1) \n",
    "      loss = loss_fn(output_train, train_batch['target'])\n",
    "      loss.requires_grad = True\n",
    "      loss_train += loss.item()\n",
    "      num_trains += 1\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_validations = 0\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in validation_data_loader:\n",
    "        output_validation = model(validation_batch['input'])\n",
    "        output_validation = torch.argmax(output_validation, dim=1) \n",
    "        loss = loss_fn(output_validation, validation_batch['target'])\n",
    "        loss.requires_grad = True\n",
    "        loss_validation += loss.item()\n",
    "        num_validations += 1\n",
    "    print(\n",
    "      f\"Epoch {epoch}, \"\n",
    "      f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "      f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "    )\n",
    "\n",
    "    wandb.log({\n",
    "      \"Epoch\": epoch,\n",
    "      \"Training loss\": loss_train / num_trains,\n",
    "      \"Validation loss\": loss_validation / num_validations\n",
    "    })\n",
    "\n",
    "  CURRENT_FILE_PATH = os.path.dirname(os.path.abspath('./hw2.ipynb'))\n",
    "  sub_path = os.path.join(CURRENT_FILE_PATH, \"gender_submission.csv\") #test.csv가 있는 경로 저장\n",
    "  submission = pd.read_csv(sub_path) #불러오기\n",
    "  with torch.no_grad():\n",
    "    for test_batch in test_data_loader:\n",
    "      output_test = model(test_batch['input'])\n",
    "      output_test = torch.argmax(output_test, dim=1) \n",
    "      print(\"[TEST]\")\n",
    "      submission['Survived'] = output_test\n",
    "      submission.to_csv('hw2.csv', index=False)\n",
    "      print(output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwer9295\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cv-05/hy/DeepLearning_assign/wandb/run-20231015_225836-2fec11a6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wer9295/hw2/runs/2fec11a6\" target=\"_blank\">2023-10-15_22-58-34</a></strong> to <a href=\"https://wandb.ai/wer9295/hw2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 4, 'batch_size': 32, 'learning_rate': 0.01, 'n_hidden_unit_list': [20, 20]}\n",
      "################################################## 1\n",
      "Epoch 1, Training loss 0.3582, Validation loss 0.3177\n",
      "Epoch 2, Training loss 0.3582, Validation loss 0.3542\n",
      "Epoch 3, Training loss 0.3549, Validation loss 0.3542\n",
      "Epoch 4, Training loss 0.3549, Validation loss 0.3177\n",
      "[TEST]\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▆█</td></tr><tr><td>Training loss</td><td>██▁▁</td></tr><tr><td>Validation loss</td><td>▁██▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>4</td></tr><tr><td>Training loss</td><td>0.35494</td></tr><tr><td>Validation loss</td><td>0.31771</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">2023-10-15_22-58-34</strong>: <a href=\"https://wandb.ai/wer9295/hw2/runs/2fec11a6\" target=\"_blank\">https://wandb.ai/wer9295/hw2/runs/2fec11a6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231015_225836-2fec11a6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "config = {\n",
    "    'epochs': 4,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_hidden_unit_list': [20, 20],\n",
    "    }\n",
    "\n",
    "wandb.init(\n",
    "    mode=\"online\" if True else \"disabled\",\n",
    "     project=\"hw2\",\n",
    "     notes=\"DeepLearning\",\n",
    "    tags=[\"my_model\", \"california_housing\"],\n",
    "    name=current_time_str,\n",
    "    config=config\n",
    "  )\n",
    "# print(args)\n",
    "print(wandb.config)\n",
    "\n",
    "my_model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "wandb.watch(my_model)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "training_loop(\n",
    "  model=my_model,\n",
    "  optimizer=optimizer,\n",
    "  train_data_loader=train_data_loader,\n",
    "  validation_data_loader=validation_data_loader,\n",
    "  test_data_loader=test_data_loader\n",
    "  )\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 리더보드 제출 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![leaderboard.PNG](./leaderboard.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 고찰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과제를 하면서 titanic 데이터셋을 분석하여 어떤 feature가 중요한지 그리고 결측치 처리 방법에 대해 알 수 있었다.  \n",
    "  \n",
    "하지만 f_my_model_training_with_wandb.py의 학습 모델과 학습하는 코드를 데이터에 맞게 수정하다 보니 오류가 많이 발생했다.  \n",
    "  \n",
    "일단 output의 dimension의 크기가 맞지 않아서 오류가 발생하여 output_test = torch.argmax(output_test, dim=1)를 추가했고 또 jupyter에서 wanb의 parser 문제가 발생하여 실행할 수 없었다. \n",
    "   \n",
    "그래서 config의 값들을 각각 선언해주었다. 또한 test 메서드에서 submission.csv를 내보내는 코드를 작성했지만 my_model = MyModel(n_input=11, n_output=2) 이 코드가 model의 객체를 다시 불러와서 \n",
    "학습이 진행되지 않고 랜덤한 값을 초기화만 했다.  \n",
    "  \n",
    "그래서 test 메서드를 없애고 학습하는 메서드에서 학스하여 나온 output값을 gender_submission.csv파일에 넣어주는 코드를 작성하였다.  \n",
    "  \n",
    "이렇게 오류를 해결하기 위해 코드를 자꾸 바꾸다 보니 교수님이 주신 코드에서 많이 바뀐 것 같다.   \n",
    "  \n",
    "그리고 epoch의 값을 여러가지 수행해 보았고 batch사이즈도 변경하여 실험을 해보았다. epoch이 6을 넘으면 overfitting이 일어나는 것을 확인 할 수 있엇다.  \n",
    "  \n",
    "그중 epoch은 4의 값이 loss가 가장 적었고 batch 사이즈는 32가 가장 loss가 적었다.  \n",
    "  \n",
    "오류를 줄이기 위해서 hidden_size를 더 늘리거나 layer의 층을 더 깊게 쌓는게 더 효율이 좋았다.\n",
    "  \n",
    "또한 더 적합한 learning_rate를 찾는다면 더욱 오류를 줄일 수 있을 것 같다.  \n",
    "  \n",
    "마지막으로 활성화 함수 ELU,Leaky ReLU, PReLUm ReLU등 여러가지 활성화 함수를 사용했는데 Leaky ReLU 활성화 함수가 가장 적은 loss를 보였다. 하지만 ReLU랑 크게 다르진 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제를 하면서 느낀 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예전에 daycon에서 주관하는 전력사용량 예측하는 대회에 참가해 본적이 있었다.  \n",
    "  \n",
    "model은 pytorch에서 불러오면 되었기 때문에 model 코드를 작성하는 부분에는 어려움이 없었다. 하지만 데이터를 분석하고 데이터를 전처리 하는 과정이 중요했었다. \n",
    "  \n",
    "loss를 줄이기 위해 여러가지 활성화 함수도 사용하고 다양한 optim, hyperparmeter를 사용하여 결과를 냈었다.  \n",
    "  \n",
    "이번 과제도 마찬가지로 그런 부분에서 kaggle이란 대회에서 제공하는 데이터를 분석하고 전처리하는 과정 또 실험하는 코드가 있었는데 재미가 있었다.  \n",
    "  \n",
    "또한 wanb라는 사이트에 가입하였고 실제로 loss가 변하는 것을 그래프로 볼 수 있다면 앞으로 DeepLearning을 하면서 hyperparameter를 실험하는 과정이 많이 있을텐데 wanb 라이브러리를 자주 사용한다면 실험을 더욱 편리하게 할 수 있을 것 같다고 느꼈고 많은 것을 배울 수 있었던 과제인 것 같다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
